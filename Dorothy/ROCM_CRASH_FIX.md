# ROCm GPU Crash Fix: "Module not initialized" / "Aborted (core dumped)"

## What Happened

Your training script is working correctly! It successfully:
âœ… Combined datasets (9 examples)
âœ… Loaded the model
âœ… Performed abliteration 
âœ… Started fine-tuning

But then **ROCm crashed** with:
```
Module not initialized
Aborted (core dumped)
```

This is a **GPU driver/runtime issue**, not a Python code bug.

## Root Causes (in order of likelihood)

1. **Memory fragmentation** - ROCm 6.4 has issues with memory allocation
2. **Multi-GPU context confusion** - Even with 1 GPU, ROCm can get confused
3. **Uninitialized HIP context** - ROCm runtime didn't initialize properly
4. **Kernel version mismatch** - PyTorch 2.8 + ROCm 6.4 compatibility issue

## Fixes Applied to train_dorothy.py

### Fix 1: ROCm Environment Variables (Top of script)
```python
os.environ["HIP_VISIBLE_DEVICES"] = "0"  # Force single GPU
os.environ["PYTORCH_HIP_ALLOC_CONF"] = "max_split_size_mb:128"  # Reduce fragmentation
os.environ["ROCM_PATH"] = "/opt/rocm"  # Explicit ROCm path
```

### Fix 2: Reduced Batch Size (TrainingArguments)
```python
per_device_train_batch_size=1,  # Reduced from 2
gradient_accumulation_steps=16,  # Increased to maintain effective batch size
dataloader_num_workers=0,  # Disable multiprocessing
dataloader_pin_memory=False,  # Disable pinned memory
```

## How to Apply

**Option 1: Use fixed script**
```bash
cp /mnt/user-data/outputs/train_dorothy.py ~/Documents/heterodyne_detector/CTK2-Dorothy/train_dorothy.py
python ~/Documents/heterodyne_detector/CTK2-Dorothy/train_dorothy.py
```

**Option 2: Skip abliteration** (if still crashing)
```bash
python train_dorothy.py --no-abliteration
```

This skips the refusal removal but still does fine-tuning.

## Additional Troubleshooting

### If still crashing, try these in order:

#### 1. Clear GPU cache before running
```bash
python -c "import torch; torch.cuda.empty_cache()"
python train_dorothy.py
```

#### 2. Use CPU for abliteration, GPU for training
Add this flag (requires code modification):
```python
# In identify_refusal_direction, change device:
model = model.to('cpu')  # Move to CPU for abliteration
# ... do abliteration ...
model = model.to('cuda')  # Move back for training
```

#### 3. Reduce max_seq_length
In `fine_tune_model`, change:
```python
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=self.base_model,
    max_seq_length=2048,  # Reduced from 4096
    dtype=torch.float16,
    load_in_4bit=True,
)
```

#### 4. Check ROCm installation
```bash
# Verify ROCm is working
rocm-smi

# Check PyTorch sees GPU
python -c "import torch; print(torch.cuda.is_available()); print(torch.cuda.get_device_name())"

# Should output:
# True
# AMD Radeon RX 6700 XT
```

#### 5. Update ROCm PyTorch (if very old)
```bash
pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.2
```

#### 6. Nuclear option: Use smaller model
Change base model to even smaller version:
```bash
python train_dorothy.py --model "Qwen/Qwen2.5-Coder-0.5B-Instruct"
```

## Known ROCm Issues with Solutions

### Issue: "expandable_segments not supported"
**Status:** Warning only, can be ignored

### Issue: "Module not initialized" during training
**Cause:** ROCm HIP context initialization failure  
**Fix:** Applied in updated script (environment variables)

### Issue: Out of memory during training
**Fix:** Reduce batch size to 1 (already applied)

### Issue: Core dump with no error message
**Cause:** GPU kernel panic  
**Fix:** Disable gradient checkpointing:
```python
use_gradient_checkpointing=False  # Change to False in get_peft_model
```

## Success Indicators

After fixes, you should see:
```
Combined 9 training examples
Loading base model: Qwen/Qwen2.5-Coder-1.5B-Instruct
Identifying refusal direction...
Refusal direction computed: shape torch.Size([1536])

Abliterating layers 8-16 with strength 0.8...
  Abliterated layer 8
  Abliterated layer 10
  Abliterated layer 12
  Abliterated layer 14
Abliteration complete

Testing refusal removal...
Refusal rate: 0/3 = 0.0%

Starting fine-tuning...
Unsloth 2026.1.4 patched 28 layers...

Training: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100% 3/3 [05:23<00:00]  âœ“ NO CRASH
Training complete!
```

## If Nothing Works

**Workaround: Train on CPU (very slow but stable)**
```bash
export CUDA_VISIBLE_DEVICES=""
python train_dorothy.py --no-abliteration
```

This will take 4-6 hours instead of 45 minutes, but it will work.

## Debug Checklist

Run these to diagnose the crash point:

```bash
# 1. Verify ROCm basics
rocm-smi
rocminfo | grep "Name:"

# 2. Check GPU is accessible
python -c "import torch; print(torch.cuda.device_count())"

# 3. Test basic GPU operation
python -c "import torch; x = torch.rand(100, 100).cuda(); print(x.sum())"

# 4. Check if it's memory related
python -c "import torch; print(torch.cuda.memory_allocated(), torch.cuda.memory_reserved())"

# 5. Test with minimal Unsloth example
python -c "from unsloth import FastLanguageModel; model, tok = FastLanguageModel.from_pretrained('unsloth/Qwen2.5-1.5B-bnb-4bit', max_seq_length=512, dtype=None, load_in_4bit=True); print('Success')"
```

If step 5 fails, the issue is with Unsloth + ROCm, not your training script.

## Technical Details of the Crash

The crash occurs at:
```
/src/clr/hipamd/src/hip_global.cpp:158
Module not initialized
```

This is in the HIP (ROCm's CUDA equivalent) initialization code. It means:
- The GPU context failed to initialize during the first training step
- Most likely cause: memory allocation failure or context corruption
- Not a Python-level bug - it's a C++ GPU driver crash

## Alternative: Cloud Training

If local ROCm continues to be unstable:

1. **Google Colab** (free, has NVIDIA GPUs)
2. **Lambda Labs** ($0.50/hr for A10)
3. **RunPod** ($0.34/hr for RTX 4090)

Upload your fixed `train_dorothy.py` and dataset files, should work immediately on NVIDIA.

---

## Summary

âœ… **Training script is correct** - passed all checks
âŒ **ROCm driver crashed** - needs environment/batch size fixes
ðŸ”§ **Fixes applied** - reduced memory usage, added stability settings
ðŸ§ª **Next step** - try updated script with conservative settings
